{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend module://ipykernel.pylab.backend_inline version unknown\n",
      "Loading training data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-272824a5a391>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-272824a5a391>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    258\u001b[0m                         \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m                         usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_positive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Oversampling positive events\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1067\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1837\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._concatenate_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m     \"\"\"\n\u001b[0;32m    480\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\"\"\"This code is based on this script by baris:\n",
    "https://www.kaggle.com/bk0000/non-blending-lightgbm-model-lb-0-977?scriptVersionId=3224614\n",
    "\"\"\"\n",
    "\n",
    "DEBUG = 0\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = [logging.StreamHandler(sys.stdout)]\n",
    "logger.setLevel(20 - DEBUG * 10)\n",
    "\n",
    "predictors=[]\n",
    "\n",
    "def do_next_Click(df, agg_suffix='nextClick', agg_type='float32'):\n",
    "    \"\"\"Extracting next click feature.\n",
    "    Taken help from https://www.kaggle.com/nanomathias/feature-engineering-importance-testing  \n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting {} time calculation features...\".format(agg_suffix))\n",
    "    \n",
    "    GROUP_BY_NEXT_CLICKS = [\n",
    "        {'groupby': ['device']},\n",
    "        {'groupby': ['device', 'channel']},     \n",
    "        {'groupby': ['app', 'device', 'channel']},\n",
    "        {'groupby': ['device', 'hour']}\n",
    "    ]\n",
    "\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "    \n",
    "       # Name of new feature\n",
    "        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n",
    "    \n",
    "        # Unique list of features to select\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        # Run calculation\n",
    "        logger.info(\">> Grouping by {}\".format(spec['groupby']))\n",
    "        df[new_feature] = (df[all_features]\n",
    "                           .groupby(spec['groupby'])\n",
    "                           .click_time.shift(-1) - df.click_time).dt.seconds.astype(agg_type)\n",
    "        predictors.append(new_feature)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_prev_Click(df, agg_suffix='prevClick', agg_type='float32'):\n",
    "    \"\"\"Extracting previous click feature.\n",
    "    Taken help from https://www.kaggle.com/nanomathias/feature-engineering-importance-testing  \n",
    "    \"\"\"\n",
    "    logger.info(\">> Extracting {} time calculation features...\".format(agg_suffix))\n",
    "    \n",
    "    GROUP_BY_NEXT_CLICKS = [\n",
    "        {'groupby': ['ip', 'channel']},\n",
    "        {'groupby': ['ip', 'os']}\n",
    "    ]\n",
    "\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "    \n",
    "       # Name of new feature\n",
    "        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n",
    "    \n",
    "        # Unique list of features to select\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        # Run calculation\n",
    "        logger.info(\">> Grouping by {}\".format(spec['groupby']))\n",
    "        df[new_feature] = (df.click_time - \n",
    "                           df[all_features]\n",
    "                           .groupby(spec['groupby'])\n",
    "                           .click_time.shift(+1)).dt.seconds.astype(agg_type)\n",
    "        \n",
    "        predictors.append(new_feature)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_count(df, group_cols, agg_type='uint32', show_max=False, show_agg=True):\n",
    "    \"\"\"Add a new column with the count of another one after \n",
    "    grouping on a set of columns.\n",
    "    \"\"\"\n",
    "    agg_name='{}count'.format('_'.join(group_cols))\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    logger.info(\"{} max value = {}\".format(agg_name, df[agg_name].max()))\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_countuniq(df, group_cols, counted, agg_type='uint32', show_max=False, show_agg=True):\n",
    "    \"\"\"Add a new column with the unique count of another one after \n",
    "    grouping on a set of columns.\n",
    "    \"\"\"\n",
    "    agg_name= '{}_by_{}_countuniq'.format(('_'.join(group_cols)),(counted))  \n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    logger.info(\"{} max value = {}\".format(agg_name, df[agg_name].max()))\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_cumcount(df, group_cols, counted,agg_type='uint32', show_max=False, show_agg=True):\n",
    "    \"\"\"Add a new column with the cumulative count of another one after \n",
    "    grouping on a set of columns.\n",
    "    \"\"\"\n",
    "    agg_name = '{}_by_{}_cumcount'.format(('_'.join(group_cols)),(counted)) \n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name] = gp.values\n",
    "    del gp\n",
    "    logger.info(\"{} max value = {}.\".format(agg_name, df[agg_name].max()))\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_mean(df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True):\n",
    "    \"\"\"Add a new column with the mean value of a another one after \n",
    "    grouping on a set of columns.\n",
    "    \"\"\"\n",
    "    agg_name= '{}_by_{}_mean'.format(('_'.join(group_cols)),(counted))  \n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    logger.info(\"{} max value = {}\".format(agg_name, df[agg_name].max()))\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_var(df, group_cols, counted, agg_type='float32', show_max=False, show_agg=True):\n",
    "    \"\"\"Add a new column with the variance value of another one after\n",
    "    grouping on a set of columns.\n",
    "    \"\"\"\n",
    "    agg_name= '{}_by_{}_var'.format(('_'.join(group_cols)),(counted)) \n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    logger.info(\"{} max value = {}\".format(agg_name, df[agg_name].max()))\n",
    "    df[agg_name] = df[agg_name].astype(agg_type)\n",
    "    predictors.append(agg_name)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target',\n",
    "                      objective='binary', metrics='auc', feval=None,\n",
    "                      early_stopping_rounds=50, num_boost_round=3000,\n",
    "                      verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.05,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values,\n",
    "                          label=dtrain[target].values,\n",
    "                          feature_name=predictors)\n",
    "    \n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values,\n",
    "                          label=dvalid[target].values,\n",
    "                          feature_name=predictors)\n",
    "    \n",
    "    del dtrain\n",
    "    del dvalid\n",
    "    gc.collect()\n",
    "    \n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    logger.info(\"Model Report\")\n",
    "    logger.info(\"bst1.best_iteration: {}\".format(bst1.best_iteration))\n",
    "    logger.info(\"{}:{}\".format(metrics, evals_results['valid'][metrics][bst1.best_iteration-1]))\n",
    "    return bst1, bst1.best_iteration\n",
    "\n",
    "\n",
    "def sample_positive(df: pd.DataFrame, positive_ratio: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Over sample positive events.\n",
    "    :param positive_ratio: The ratio of positive events to maintain.\n",
    "    :return: Over sampled `DataFrame`.\n",
    "    \"\"\"\n",
    "    positive = df[df.is_attributed == 1]  # Select positive events\n",
    "    negative = df[df.is_attributed == 0]  # And negative events\n",
    "    \n",
    "    negative_sampled = negative.sample(len(positive) * 9)  # Sample negative events with negative 1 : 9 positive ratio\n",
    "    logger.info('Sampled data: {:,} positive, {:,} => {:,} negative.'\n",
    "                .format(positive.shape[0], negative.shape[0], negative_sampled.shape[0]))\n",
    "    return (positive\n",
    "            .append(negative_sampled)\n",
    "            .sort_values(by='click_time')\n",
    "            .reset_index(drop=True))  # Combine negative and positive \n",
    "\n",
    "def main():\n",
    "    dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint8',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "    }\n",
    "\n",
    "    logger.debug('*** Running in DEBUG mode. ***')\n",
    "    nrows = 100000 if logger.getEffectiveLevel() == logging.DEBUG else None\n",
    "    \n",
    "    logger.info(\"Loading training data...\")\n",
    "    train = pd.read_csv('../input/train.csv',\n",
    "                        parse_dates=['click_time'],\n",
    "                        nrows=nrows,\n",
    "                        dtype=dtypes,\n",
    "                        usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "    \n",
    "    train_df = sample_positive(train) # Oversampling positive events\n",
    "    del(train)\n",
    "        \n",
    "    logger.info('Loading test data...')\n",
    "    test_df = pd.read_csv(\"../input/test.csv\",\n",
    "                          nrows=nrows,\n",
    "                          parse_dates=['click_time'],\n",
    "                          dtype=dtypes,\n",
    "                          usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "    train_size = len(train_df)\n",
    "    val_size = int(train_size * 0.4)\n",
    "\n",
    "    all_df = train_df.append(test_df).reset_index(drop=True)\n",
    "    del test_df\n",
    "\n",
    "    gc.collect()\n",
    "    all_df['hour'] = pd.to_datetime(all_df.click_time).dt.hour.astype('int8')\n",
    "    all_df['day'] = pd.to_datetime(all_df.click_time).dt.day.astype('int8') \n",
    "    all_df = do_next_Click(all_df, agg_suffix='nextClick', agg_type='float32'); gc.collect()\n",
    "    \n",
    "    all_df = do_countuniq(all_df, ['device'], 'day'); gc.collect()\n",
    "    all_df = do_var(all_df, ['device'], 'day'); gc.collect()\n",
    "    \n",
    "    all_df = do_countuniq(all_df, ['device', 'day'], 'hour'); gc.collect()\n",
    "    all_df = do_var(all_df, ['device', 'day'], 'hour'); gc.collect()\n",
    "    \n",
    "    all_df = do_countuniq(all_df, ['channel', 'day'], 'hour'); gc.collect()\n",
    "    all_df = do_var(all_df, ['channel', 'day'], 'hour'); gc.collect()\n",
    "\n",
    "    all_df = do_countuniq(all_df, ['ip', 'channel'], 'app'); gc.collect()\n",
    "    all_df = do_countuniq(all_df, ['ip'], 'device'); gc.collect()\n",
    "        \n",
    "    all_df = do_count(all_df, ['ip', 'day', 'hour']); gc.collect()\n",
    "    \n",
    "    all_df = do_mean(all_df, ['ip', 'app', 'channel'], 'hour'); gc.collect()\n",
    "    \n",
    "    del all_df['day']\n",
    "    gc.collect()\n",
    "    \n",
    "    logger.info('Before appending predictors...{}'.format(sorted(predictors)))\n",
    "    target = 'is_attributed'\n",
    "    word = ['app','device','os', 'channel', 'hour']\n",
    "    for feature in word:\n",
    "        if feature not in predictors:\n",
    "            predictors.append(feature)\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "    logger.info('After appending predictors...{}'.format(sorted(predictors)))\n",
    "\n",
    "    train_df = all_df.iloc[:(train_size - val_size)]\n",
    "    val_df = all_df.iloc[(train_size - val_size) : train_size]\n",
    "    test_df = all_df.iloc[train_size:]\n",
    "\n",
    "    logger.info(\"Training size: {}\".format(len(train_df)))\n",
    "    logger.info(\"Validation size: {}\".format(len(val_df)))\n",
    "    logger.info(\"Test size : {}\".format(len(test_df)))\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "    gc.collect()\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.10,\n",
    "        #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "        'num_leaves': 7,  # 2^max_depth - 1\n",
    "        'max_depth': 3,  # -1 means no limit\n",
    "        'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight':200 # because training data is extremely unbalanced \n",
    "    }\n",
    "    \n",
    "    bst, best_iteration = lgb_modelfit_nocv(params,\n",
    "                                            train_df,\n",
    "                                            val_df,\n",
    "                                            predictors,\n",
    "                                            target,\n",
    "                                            objective='binary',\n",
    "                                            metrics='auc',\n",
    "                                            early_stopping_rounds=30,\n",
    "                                            verbose_eval=True,\n",
    "                                            num_boost_round=1000,\n",
    "                                            categorical_features=categorical)\n",
    "\n",
    "    logger.info('[{}]: model training time'.format(time.time() - start_time))\n",
    "    del train_df\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "\n",
    "    ax = lgb.plot_importance(bst, max_num_features=300)\n",
    "    plt.show()\n",
    "\n",
    "    logger.info(\"Predicting...\")\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors], num_iteration=best_iteration)\n",
    "    sub.to_csv('sub_{}.csv'.format(str(int(time.time()))), index=False, float_format='%.9f')\n",
    "    logger.info(\"Done...\")\n",
    "    return sub\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
