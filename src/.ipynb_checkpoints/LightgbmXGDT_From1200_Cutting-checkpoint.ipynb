{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "train size:  37500000\n",
      "valid size:  2500000\n",
      "test size :  18790469\n",
      "Training...\n",
      "preparing validation datasets\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e25399f7bce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mfvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mftest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mDO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e25399f7bce2>\u001b[0m in \u001b[0;36mDO\u001b[1;34m(ftrain, fvalid, ftest)\u001b[0m\n\u001b[0;32m    167\u001b[0m                             \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                             \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                             categorical_features=categorical)\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[{}]: model training time'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e25399f7bce2>\u001b[0m in \u001b[0;36mlgb_modelfit_nocv\u001b[1;34m(params, dtrain, dvalid, predictors, target, objective, metrics, feval, early_stopping_rounds, num_boost_round, verbose_eval, categorical_features)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preparing validation datasets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n\u001b[0m\u001b[0;32m     46\u001b[0m                           \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                           \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3809\u001b[0m         \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mflot64\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3810\u001b[0m         \"\"\"\n\u001b[1;32m-> 3811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3813\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mas_matrix\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mas_matrix\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m   3676\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3677\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3678\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3680\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_interleaved_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3687\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# debug = 0 if submission else debug \n",
    "debug= 0\n",
    "if debug:\n",
    "    print('*** debug parameter set: this is a test run for debugging purposes ***')  \n",
    "    \n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.15,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 15,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': 5,  # -1 means no limit\n",
    "        'min_child_samples': 180,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7839190373652398,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.8707045749521953,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "    del xgtrain, xgvalid\n",
    "    gc.collect()\n",
    "    return (bst1,bst1.best_iteration)\n",
    "\n",
    "def DO(ftrain, fvalid, ftest):\n",
    "    dtypes = {\n",
    "            'ip'            : 'uint32',\n",
    "            'app'           : 'uint16',\n",
    "            'device'        : 'uint16',\n",
    "            'os'            : 'uint16',\n",
    "            'channel'       : 'uint16',\n",
    "            'is_attributed' : 'uint8',\n",
    "            'click_id'      : 'uint32',\n",
    "            'hour'          : 'uint8',\n",
    "            'day'           : 'uint8',\n",
    "            'minute'        : 'uint8',\n",
    "            'X0'            : 'int64',\n",
    "            'X1'            : 'int64',\n",
    "            'X2'            : 'int64',\n",
    "            'X3'            : 'int64',\n",
    "            'X4'            : 'int64',\n",
    "            'X5'            : 'int64',\n",
    "            'X6'            : 'int64',\n",
    "            'X7'            : 'int64',\n",
    "            'X8'            : 'int64',\n",
    "            'devicenext_Clicl' : 'float32',\n",
    "            'device_channel_nextClick' : 'float32',\n",
    "            'app_device_channel_nextClick': 'float32',\n",
    "            'device_hour_nextClick': 'float32',\n",
    "            'ip_channel_prevClick': 'float32',\n",
    "            'ip_os_prevClick': 'float32',\n",
    "            'nextClick': 'int64',\n",
    "            'nextClick_shift': 'float64',\n",
    "            'nextClick_shift_shift': 'float64',\n",
    "            'nextClick_shiftback': 'float64',\n",
    "            'ip_tcount': 'int64',\n",
    "            'ip_app_count': 'int64',\n",
    "            'ip_app_os_count': 'int64',\n",
    "            'ip_tchan_count': 'float64',\n",
    "            'ip_app_os_var': 'float64',\n",
    "            'ip_app_channel_var_day': 'float64',\n",
    "            'ip_app_channel_mean_hour': 'float64'\n",
    "            }\n",
    "\n",
    "    predictors = ['device_nextClick', 'device_channel_nextClick', 'app_device_channel_nextClick', 'device_hour_nextClick',\n",
    "                  'nextClick', 'nextClick_shift',\n",
    "                  'nextClick_shiftback', 'app', 'device', 'os', 'channel', 'hour', 'ip_tcount',\n",
    "                  'ip_app_count', 'ip_app_os_count', 'ip_app_os_var', 'ip_app_channel_mean_hour',\n",
    "                  'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "    usecols = ['device_nextClick', 'device_channel_nextClick', 'app_device_channel_nextClick', 'device_hour_nextClick',\n",
    "                  'ip_channel_prevClick', 'ip_os_prevClick', 'nextClick', 'nextClick_shift', 'nextClick_shift_shift',\n",
    "                  'nextClick_shiftback', 'app', 'device', 'os', 'channel', 'hour', 'day', 'ip_tcount', 'ip_tchan_count',\n",
    "                  'ip_app_count', 'ip_app_os_count', 'ip_app_os_var', 'ip_app_channel_var_day', 'ip_app_channel_mean_hour',\n",
    "                  'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'is_attributed', 'click_id']\n",
    "    usecolstest = ['device_nextClick', 'device_channel_nextClick', 'app_device_channel_nextClick', 'device_hour_nextClick',\n",
    "                  'ip_channel_prevClick', 'ip_os_prevClick', 'nextClick', 'nextClick_shift', 'nextClick_shift_shift',\n",
    "                  'nextClick_shiftback', 'app', 'device', 'os', 'channel', 'hour', 'day', 'ip_tcount', 'ip_tchan_count',\n",
    "                  'ip_app_count', 'ip_app_os_count', 'ip_app_os_var', 'ip_app_channel_var_day', 'ip_app_channel_mean_hour',\n",
    "                  'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "\n",
    "    target = 'is_attributed'\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "    \n",
    "    print('loading train data...')\n",
    "    train_df = pd.read_csv(ftrain, usecols=usecols)\n",
    "    val_df = pd.read_csv(fvalid, usecols=usecols)\n",
    "    test_df  = pd.read_csv(ftest, usecols=usecolstest)\n",
    "    \n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "    print(\"test size : \", len(test_df))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.15,\n",
    "        #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "        'num_leaves': 15,  # 2^max_depth - 1\n",
    "        'max_depth': 5,  # -1 means no limit\n",
    "        'min_child_samples': 185,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7839190373652398,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.8707045749521953,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight':180.53004597457985 # because training data is extremely unbalanced \n",
    "    }\n",
    "    (bst,best_iteration) = lgb_modelfit_nocv(params, \n",
    "                            train_df, \n",
    "                            val_df, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            objective='binary', \n",
    "                            metrics='auc',\n",
    "                            early_stopping_rounds=30, \n",
    "                            verbose_eval=True, \n",
    "                            num_boost_round=1000, \n",
    "                            categorical_features=categorical)\n",
    "\n",
    "    print('[{}]: model training time'.format(time.time() - start_time))\n",
    "    from datetime import datetime as dt\n",
    "    datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "    subt = pd.DataFrame()\n",
    "    subt['is_attributed'] = bst.predict(train_df[predictors], num_iteration=best_iteration)\n",
    "    fname = 'sub_dart_train_BayesOpt' + datetime + '.csv'\n",
    "    #subt.to_csv(fname, index=False)\n",
    "    del subt\n",
    "    gc.collect()\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    subt = pd.DataFrame()\n",
    "    subt['is_attributed'] = bst.predict(val_df[predictors], num_iteration=best_iteration)\n",
    "    fname = 'sub_xg_valid_BayesOpt' + datetime +  '.csv'\n",
    "    #subt.to_csv(fname, index=False)\n",
    "    del subt; gc.collect()\n",
    "    del val_df; gc.collect()\n",
    "    \n",
    "    print('Plot feature importances...')\n",
    "    ax = lgb.plot_importance(bst, max_num_features=100)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    sub = pd.DataFrame()\n",
    "    click_id = pd.read_csv(\"../input/test.csv\", usecols=['click_id'])\n",
    "    sub['click_id'] = click_id.astype('int')\n",
    "    del click_id; gc.collect()\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    from datetime import datetime as dt\n",
    "    datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "    if not debug:\n",
    "        print(\"writing...\")\n",
    "        fname = 'sub_xgdt_BayesOptCutting' + datetime +  '.csv'\n",
    "        sub.to_csv(fname, index=False)\n",
    "    print(\"done...\")\n",
    "    del bst, sub\n",
    "    gc.collect()\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "\n",
    "pathlist = [['./rawdata_train_From12000_2018_0503_0151_25.csv', './rawdata_valid_From12000_2018_0503_0151_25.csv', './Dataset_xgdt_test_From1200_2018_0503_0215_36.csv'],\n",
    "            ['./rawdata_train_From12000_2018_0503_0410_16.csv', './rawdata_valid_From12000_2018_0503_0410_16.csv', './Dataset_xgdt_test_From1200_2018_0503_0436_26.csv'],\n",
    "            ['./rawdata_train_From12000_2018_0503_0622_23.csv', './rawdata_valid_From12000_2018_0503_0622_23.csv', './Dataset_xgdt_test_From1200_2018_0503_0646_04.csv'],\n",
    "            ['./rawdata_train_From12000_2018_0503_0843_57.csv', './rawdata_valid_From12000_2018_0503_0843_57.csv', './Dataset_xgdt_test_From1200_2018_0503_0910_04.csv']\n",
    "           ]\n",
    "for paths in pathlist:\n",
    "    ftrain = paths[0]\n",
    "    fvalid = paths[1]\n",
    "    ftest = paths[2]\n",
    "    DO(ftrain, fvalid, ftest)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
