{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data... 24000000 64000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[1;34m(*date_cols)\u001b[0m\n\u001b[0;32m   3021\u001b[0m                     \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3022\u001b[1;33m                     \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3023\u001b[0m                 )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[1;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mis_datetime64_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8896081979d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[0mto\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfrm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m     \u001b[0mDO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8896081979d4>\u001b[0m in \u001b[0;36mDO\u001b[1;34m(frm, to, fileno)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loading train data...'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfrm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes,\n\u001b[1;32m--> 152\u001b[1;33m                            usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loading test data...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1067\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1914\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1916\u001b[1;33m             \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1917\u001b[0m             \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_do_date_conversions\u001b[1;34m(self, names, data)\u001b[0m\n\u001b[0;32m   1665\u001b[0m             data, names = _process_date_conversion(\n\u001b[0;32m   1666\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_date_conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1667\u001b[1;33m                 self.index_names, names, keep_date_col=self.keep_date_col)\n\u001b[0m\u001b[0;32m   1668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_process_date_conversion\u001b[1;34m(data_dict, converter, parse_spec, index_col, index_names, columns, keep_date_col)\u001b[0m\n\u001b[0;32m   3073\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m_isindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m                 \u001b[0mdata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolspec\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolspec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3076\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m                 new_name, col, old_names = _try_convert_dates(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[1;34m(*date_cols)\u001b[0m\n\u001b[0;32m   3024\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m                 return tools.to_datetime(\n\u001b[1;32m-> 3026\u001b[1;33m                     parsing.try_parse_dates(strs, dayfirst=dayfirst))\n\u001b[0m\u001b[0;32m   3027\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.try_parse_dates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.try_parse_dates.lambda\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1310\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m                                                       second=0, microsecond=0)\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipped_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[0myearfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_timelex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# Splits the timestr into tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__slots__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# debug = 0 if submission else debug \n",
    "debug= 0\n",
    "if debug:\n",
    "    print('*** debug parameter set: this is a test run for debugging purposes ***')\n",
    "\n",
    "def do_next_Click(df, predictors, agg_suffix='nextClick', agg_type='float32'):\n",
    "    \"\"\"Extracting next click feature.\n",
    "    Taken help from https://www.kaggle.com/nanomathias/feature-engineering-importance-testing  \n",
    "    \"\"\"\n",
    "    \n",
    "    GROUP_BY_NEXT_CLICKS = [\n",
    "        {'groupby': ['device']},\n",
    "        {'groupby': ['device', 'channel']},     \n",
    "        {'groupby': ['app', 'device', 'channel']},\n",
    "        {'groupby': ['device', 'hour']}\n",
    "    ]\n",
    "\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "    \n",
    "       # Name of new feature\n",
    "        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n",
    "    \n",
    "        # Unique list of features to select\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        # Run calculation\n",
    "        df[new_feature] = (df[all_features]\n",
    "                           .groupby(spec['groupby'])\n",
    "                           .click_time.shift(-1) - df.click_time).dt.seconds.astype(agg_type)\n",
    "        predictors.append(new_feature)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_prev_Click(df, predictors, agg_suffix='prevClick', agg_type='float32'):\n",
    "    \"\"\"Extracting previous click feature.\n",
    "    Taken help from https://www.kaggle.com/nanomathias/feature-engineering-importance-testing  \n",
    "    \"\"\"\n",
    "    \n",
    "    GROUP_BY_NEXT_CLICKS = [\n",
    "        {'groupby': ['ip', 'channel']},\n",
    "        {'groupby': ['ip', 'os']}\n",
    "    ]\n",
    "\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in GROUP_BY_NEXT_CLICKS:\n",
    "    \n",
    "       # Name of new feature\n",
    "        new_feature = '{}_{}'.format('_'.join(spec['groupby']),agg_suffix)    \n",
    "    \n",
    "        # Unique list of features to select\n",
    "        all_features = spec['groupby'] + ['click_time']\n",
    "\n",
    "        # Run calculation\n",
    "        df[new_feature] = (df.click_time - \n",
    "                           df[all_features]\n",
    "                           .groupby(spec['groupby'])\n",
    "                           .click_time.shift(+1)).dt.seconds.astype(agg_type)\n",
    "        \n",
    "        predictors.append(new_feature)\n",
    "        gc.collect()\n",
    "    return df  \n",
    "    \n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.15,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 15,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': 5,  # -1 means no limit\n",
    "        'min_child_samples': 180,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7839190373652398,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.8707045749521953,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "\n",
    "    if not debug:\n",
    "        from datetime import datetime as dt\n",
    "        datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "        del xgtrain, xgvalid\n",
    "        gc.collect()\n",
    "        fname = 'LgbModel_dart_bestiter' + str(bst1.best_iteration) +  '.bin'\n",
    "        bst1.save_model(fname)\n",
    "    \n",
    "    return (bst1,bst1.best_iteration)\n",
    "\n",
    "def DO(frm,to,fileno):\n",
    "    dtypes = {\n",
    "            'ip'            : 'uint32',\n",
    "            'app'           : 'uint16',\n",
    "            'device'        : 'uint16',\n",
    "            'os'            : 'uint16',\n",
    "            'channel'       : 'uint16',\n",
    "            'is_attributed' : 'uint8',\n",
    "            'click_id'      : 'uint32',\n",
    "            }\n",
    "\n",
    "    print('loading train data...',frm,to)\n",
    "    train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes,\n",
    "                           usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "    print('loading test data...')\n",
    "    if debug:\n",
    "        test_df = pd.read_csv(\"../input/test.csv\", nrows=100000, parse_dates=['click_time'], dtype=dtypes,\n",
    "                              usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"../input/test.csv\", parse_dates=['click_time'], dtype=dtypes,\n",
    "                              usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "    len_train = len(train_df)\n",
    "    train_df=train_df.append(test_df)\n",
    "\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Extracting new features...')\n",
    "    train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "    train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "    train_df['minute'] = pd.to_datetime(train_df.click_time).dt.minute.astype('uint8')\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    naddfeat=9\n",
    "    for i in range(0,naddfeat):\n",
    "        if i==0: selcols=['ip', 'channel']; QQ=4;\n",
    "        if i==1: selcols=['ip', 'device', 'os', 'app']; QQ=5;\n",
    "        if i==2: selcols=['ip', 'day', 'hour']; QQ=4;\n",
    "        if i==3: selcols=['ip', 'app']; QQ=4;\n",
    "        if i==4: selcols=['ip', 'app', 'os']; QQ=4;\n",
    "        if i==5: selcols=['ip', 'device']; QQ=4;\n",
    "        if i==6: selcols=['app', 'channel']; QQ=4;\n",
    "        if i==7: selcols=['ip', 'os']; QQ=5;\n",
    "        if i==8: selcols=['ip', 'device', 'os', 'app']; QQ=4;\n",
    "        print('selcols',selcols,'QQ',QQ)\n",
    "        \n",
    "        filename='X%d_%d_%d.csv'%(i,frm,to)\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            if QQ==5: \n",
    "                gp=pd.read_csv(filename,header=None)\n",
    "                train_df['X'+str(i)]=gp\n",
    "            else: \n",
    "                gp=pd.read_csv(filename)\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "        else:\n",
    "            if QQ==0:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].count().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==1:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].mean().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==2:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].var().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==3:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].skew().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==4:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].nunique().reset_index().\\\n",
    "                    rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "                train_df = train_df.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            if QQ==5:\n",
    "                gp = train_df[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].cumcount()\n",
    "                train_df['X'+str(i)]=gp.values\n",
    "            \n",
    "            if not debug:\n",
    "                 gp.to_csv(filename,index=False)\n",
    "            \n",
    "        del gp\n",
    "        gc.collect()    \n",
    "\n",
    "    print('doing nextClick')\n",
    "    predictors=[]\n",
    "    \n",
    "    new_feature = 'nextClick'\n",
    "    filename='nextClick_%d_%d.csv'%(frm,to)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print('loading from save file')\n",
    "        QQ=pd.read_csv(filename).values\n",
    "    else:\n",
    "        D=2**26\n",
    "        train_df['category'] = (train_df['ip'].astype(str) + \"_\" + train_df['app'].astype(str) + \"_\" + train_df['device'].astype(str) \\\n",
    "            + \"_\" + train_df['os'].astype(str)).apply(hash) % D\n",
    "        click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "        train_df['epochtime']= train_df['click_time'].astype(np.int64) // 10 ** 9\n",
    "        next_clicks= []\n",
    "        for category, t in zip(reversed(train_df['category'].values), reversed(train_df['epochtime'].values)):\n",
    "            next_clicks.append(click_buffer[category]-t)\n",
    "            click_buffer[category]= t\n",
    "        del(click_buffer)\n",
    "        QQ= list(reversed(next_clicks))\n",
    "\n",
    "        if not debug:\n",
    "            print('saving')\n",
    "            pd.DataFrame(QQ).to_csv(filename,index=False)\n",
    "\n",
    "    train_df = do_next_Click(train_df, predictors, agg_suffix='nextClick', agg_type='float32'); gc.collect()\n",
    "    train_df = do_prev_Click(train_df, predictors, agg_suffix='prevClick', agg_type='float32'); gc.collect()\n",
    "        \n",
    "    train_df[new_feature] = QQ\n",
    "    predictors.append(new_feature)\n",
    "\n",
    "    train_df[new_feature+'_shift'] = pd.DataFrame(QQ).shift(+1).values\n",
    "    predictors.append(new_feature+'_shift')\n",
    "\n",
    "    train_df[new_feature+'_shift'+'_shift'] = pd.DataFrame(QQ).shift(+2).values\n",
    "    predictors.append(new_feature+'_shift'+'_shift')\n",
    "\n",
    "    train_df[new_feature+'_shiftback'] = pd.DataFrame(QQ).shift(-1).values\n",
    "    predictors.append(new_feature+'_shiftback')\n",
    "    \n",
    "    # train_df[new_feature+'_shiftback_shiftback'] = pd.DataFrame(QQ).shift(-2).values\n",
    "    # predictors.append(new_feature+'_shiftback_shiftback')\n",
    "    \n",
    "    del QQ\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-day-hour combination...')\n",
    "    gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_tcount'})\n",
    "    train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-app combination...')\n",
    "    gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by ip-app-os combination...')\n",
    "    gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    # Adding features with var and mean hour (inspired from nuhsikander's script)\n",
    "    print('grouping by : ip_day_chl_var_hour')\n",
    "    gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','channel'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_tchan_count'})\n",
    "    train_df = train_df.merge(gp, on=['ip','day','channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_os_var_hour')\n",
    "    gp = train_df[['ip','app', 'os', 'hour']].groupby(by=['ip', 'app', 'os'])[['hour']].var().reset_index().rename(index=str, columns={'hour': 'ip_app_os_var'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_channel_var_day')\n",
    "    gp = train_df[['ip','app', 'channel', 'day']].groupby(by=['ip', 'app', 'channel'])[['day']].var().reset_index().rename(index=str, columns={'day': 'ip_app_channel_var_day'})\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print('grouping by : ip_app_chl_mean_hour')\n",
    "    gp = train_df[['ip','app', 'channel','hour']].groupby(by=['ip', 'app', 'channel'])[['hour']].mean().reset_index().rename(index=str, columns={'hour': 'ip_app_channel_mean_hour'})\n",
    "    print(\"merging...\")\n",
    "    train_df = train_df.merge(gp, on=['ip','app', 'channel'], how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"vars and data type: \")\n",
    "    train_df.info()\n",
    "    train_df['ip_tcount'] = train_df['ip_tcount'].astype('uint16')\n",
    "    train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "    train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')\n",
    "\n",
    "    target = 'is_attributed'\n",
    "    predictors.extend(['app','device','os', 'channel', 'hour', 'day',\n",
    "                  'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "                  'ip_app_os_count', 'ip_app_os_var',\n",
    "                  'ip_app_channel_var_day','ip_app_channel_mean_hour'])\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "    for i in range(0,naddfeat):\n",
    "        predictors.append('X'+str(i))\n",
    "        \n",
    "    print('predictors',predictors)\n",
    "\n",
    "    test_df = train_df[len_train:]\n",
    "    val_df = train_df[(len_train-val_size):len_train]\n",
    "    train_df = train_df[:(len_train-val_size)]\n",
    "\n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "    print(\"test size : \", len(test_df))\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.15,\n",
    "        #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "        'num_leaves': 15,  # 2^max_depth - 1\n",
    "        'max_depth': 5,  # -1 means no limit\n",
    "        'min_child_samples': 185,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7839190373652398,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.8707045749521953,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight':180.53004597457985 # because training data is extremely unbalanced \n",
    "    }\n",
    "    (bst,best_iteration) = lgb_modelfit_nocv(params, \n",
    "                            train_df, \n",
    "                            val_df, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            objective='binary', \n",
    "                            metrics='auc',\n",
    "                            early_stopping_rounds=30, \n",
    "                            verbose_eval=True, \n",
    "                            num_boost_round=1000, \n",
    "                            categorical_features=categorical)\n",
    "\n",
    "    print('[{}]: model training time'.format(time.time() - start_time))\n",
    "    from datetime import datetime as dt\n",
    "    datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "    subt = pd.DataFrame()\n",
    "    subt['is_attributed'] = bst.predict(train_df[predictors], num_iteration=best_iteration)\n",
    "    fname = 'sub_dart_train_BayesOpt' + datetime + '.csv'\n",
    "    subt.to_csv(fname, index=False)\n",
    "    del subt\n",
    "    gc.collect()\n",
    "    fname = 'rawdata_train_From2400' + datetime + '.csv'\n",
    "    train_df[predictors].to_csv(fname,index=False)\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    subt = pd.DataFrame()\n",
    "    subt['is_attributed'] = bst.predict(val_df[predictors], num_iteration=best_iteration)\n",
    "    fname = 'sub_dart_valid_BayesOpt' + datetime +  '.csv'\n",
    "    subt.to_csv(fname, index=False)\n",
    "    del subt; gc.collect()\n",
    "    fname = 'rawdata_valid_From2400' + datetime + '.csv'\n",
    "    val_df[predictors].to_csv(fname, index=False)\n",
    "    del val_df; gc.collect()\n",
    "    \n",
    "    print('Plot feature importances...')\n",
    "    ax = lgb.plot_importance(bst, max_num_features=100)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    from datetime import datetime as dt\n",
    "    datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "    if not debug:\n",
    "        print(\"writing...\")\n",
    "        fname = 'sub_dart_BayesOpt' + datetime +  '.csv'\n",
    "        sub.to_csv(fname, index=False)\n",
    "    print(\"done...\")\n",
    "    del bst, sub\n",
    "    gc.collect()\n",
    "    if not debug or debug:\n",
    "        fname = 'Dataset_dart_test_' + datetime + '.csv'\n",
    "        test_df[predictors].to_csv(fname)\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "\n",
    "nrows=184903891-1\n",
    "nchunk=40000000\n",
    "val_size=2500000\n",
    "\n",
    "for numiter in range(0, 4):\n",
    "    frm = 24000000 + nchunk * numiter\n",
    "    if debug:\n",
    "        nchunk=100000\n",
    "        frm = 0\n",
    "        val_size=10000\n",
    "    \n",
    "    to= frm + nchunk\n",
    "    DO(frm, to, 0)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
