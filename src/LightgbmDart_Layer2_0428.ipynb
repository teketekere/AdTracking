{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahega\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-039effc4890a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[0mto\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfrm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0mDO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-039effc4890a>\u001b[0m in \u001b[0;36mDO\u001b[1;34m(frm, to, fileno)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mlgb_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mxgtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n\u001b[0;32m     84\u001b[0m                           \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# debug = 0 if submission else debug \n",
    "debug= 0\n",
    "if debug:\n",
    "    print('*** debug parameter set: this is a test run for debugging purposes ***')\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "    \n",
    "    from datetime import datetime as dt\n",
    "    datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "    fname = 'Dataset_dart_train' + datetime +  '.bin'\n",
    "    xgtrain.save_binary(fname)\n",
    "    fname = 'Dataset_dart_valid' + datetime +  '.bin'\n",
    "    xgvalid.save_binary(fname)\n",
    "    \n",
    "    del xgtrain, xgvalid\n",
    "    gc.collect()\n",
    "    \n",
    "    fname = 'LgbModel_dart_bestiter' + str(bst1.best_iteration) +  '.bin'\n",
    "    bst1.save_model(fname)\n",
    "    return (bst1,bst1.best_iteration)\n",
    "\n",
    "def DO(frm,to,fileno):\n",
    "    params = {\n",
    "        'learning_rate': 0.12,\n",
    "        #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "        'num_leaves': 6,  # 2^max_depth - 1\n",
    "        'max_depth': 3,  # -1 means no limit\n",
    "        'min_child_samples': 50,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.90000000000000002,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.61132455959295295,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight':136.30022681376147, # because training data is extremely unbalanced \n",
    "        'reg_alpha': 1.0\n",
    "    }\n",
    "    \n",
    "    lgb_params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.12,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 6,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': 3,  # -1 means no limit\n",
    "        'min_child_samples': 50,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.61132455959295295,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 1.0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "    \n",
    "    xgtrain = lgb.Dataset('./Dataset_dart_train_2018_0427_1005_21.bin')    \n",
    "    xgvalid = lgb.Dataset('./Dataset_dart_valid_2018_0427_1005_21.bin')\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.Booster(lgb_params, xgtrain)\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=3000,\n",
    "                     early_stopping_rounds=50,\n",
    "                     verbose_eval=10, \n",
    "                     feval=None)\n",
    "\n",
    "    xgtrain_temp = lgb.Dataset('./Dataset_dart_valid_2018_0427_1246_51.bin')\n",
    "    \n",
    "    print('Plot feature importances...')\n",
    "    ax = lgb.plot_importance(bst, max_num_features=100)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    from datetime import datetime as dt\n",
    "    datetime = dt.now().strftime('_%Y_%m%d_%H%M_%S')\n",
    "    if not debug:\n",
    "        print(\"writing...\")\n",
    "        fname = 'sub_it_DartLayer2' + datetime +  '.csv.gz'\n",
    "        sub.to_csv(fname,index=False,compression='gzip')\n",
    "    print(\"done...\")\n",
    "    del bst, sub\n",
    "    gc.collect()\n",
    "\n",
    "nrows=184903891-1\n",
    "nchunk=40000000\n",
    "val_size=2500000\n",
    "\n",
    "for numiter in range(0, 1):\n",
    "    frm = 24000000 + nchunk * numiter\n",
    "    if debug:\n",
    "        nchunk=100000\n",
    "        frm = 0\n",
    "        val_size=10000\n",
    "    \n",
    "    to= frm + nchunk\n",
    "    DO(frm, to, 0)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
